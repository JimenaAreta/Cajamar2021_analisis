{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"[Cajamar 2021] Codigo_Scraping_Datacrop.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"constant-sheet"},"source":["![image.png](attachment:image.png)\n","\n","\n","\n","\n","\n","# TÉCNICAS DE SCRAPING APLICADAS PARA ENRIQUECER LOS DATOS Y OBTENER NUEVAS FUENTES PARA EL RETO.\n","\n","\n","Para realizar scraping se ha usado la libreria selenium a través de las etiquetas XPATH y wget para descargar los informes coyunturales\n","\n","\n","## El código se divide en tres partes:\n","\n","### - 1. Setup y preparación de la libreria Selenium:\n","\n","![image-2.png](attachment:image-2.png)\n","\n","\n","Necesitamos saber la versión de chrome que tenemos para descarganos el chrome driver correcto para ejecutar selenium que debe de descargar aqui https://chromedriver.chromium.org/downloads . Se descargará la versión correspondiente con la que hemos comprobado en chrome y se descargará. Encontraremos un archivo que descomprimir donde dentro se aloja el \"Chromedriver.exe\" lo deberemos de dejar en el mismo path donde se encuentre este jupyter notebook.\n","\n","\n","Ejecutar en nuestra consola de Anaconda Prompt / CMD el siguiente comando: pip install selenium\n","\n","\n","### - 2. Ejecución de la extracción de informes coyunturales con wget y selenium:\n","\n","Simplemente deberemos ejecutar en nuestra consola de Anaconda Prompt / CMD el siguiente comando: pip install wget\n","\n","\n","### - 3. Ejecución de la extracción de datos de consumo del MAPA 2013-2020:\n","\n"," Primero se encuentra un código generador de URLS a procesar iterando sobre el numero de periodos y el tipo de producto a extraer (Hortalizas o frutas) además de despues recorrer todas estas url con selenium scrapeando las tablas del MAPA y guardando y etiquetando los datos en un Pandas Dataframe para su posterior guardado en CSV.\n","\n"],"id":"constant-sheet"},{"cell_type":"code","metadata":{"id":"entertaining-attachment"},"source":["### Selenium Set-up\n","from selenium import webdriver\n","from selenium.webdriver.support.ui import Select, WebDriverWait\n","import pandas as pd\n","pd.options.mode.chained_assignment = None\n","import time\n","import wget\n","\n","#El driver es el navegador brave basado en el motor chromium (editar para probar)\n","\n","### indicamos el path al lugar donde se encuentra el \"chromedriver.exe\"\n","\n","driver_path = \"C:/Users/Sergio/Desktop/scrapper_dcrop/chromedriver.exe\" ## Ejemplo\n","\n","## indicamos el path donde se encuentra instalado nuestro google chrome indicando el archivo \"chrome.exe\"\n","\n","chrome_path = \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" ## Ejemplo\n","\n","option = webdriver.ChromeOptions()\n","option.binary_location = chrome_path\n","#option.add_argument(\"--incognito\") OPCIONAL abrir sesion de selenium en modo incognito\n","#option.add_argument(\"--headless\") \n","\n","# Creamos una nueva instancia de Selenium y se deberá abrir una nueva ventana de Chrome\n","browser = webdriver.Chrome(executable_path=driver_path, options=option)"],"id":"entertaining-attachment","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blessed-immunology"},"source":["## Scraping variacion de precios F&H semanal"],"id":"blessed-immunology"},{"cell_type":"code","metadata":{"id":"chief-adobe"},"source":["## Scraper informes coyunturales \n","\n","# Link a los informes conyuturales para crear un dataset con la variacion de precios de F&H semanalmente\n","\n","url2 = 'https://www.mapa.gob.es/es/estadistica/temas/publicaciones/informe-semanal-coyuntura/2020.aspx' \n","\n","browser.get(url2)\n","\n","# Para guardar los links\n","links = []\n","# Bucle para obtener los links de la pagina que descargan los archivos semanales\n","for i in range(2,107,2):\n","    element = browser.find_element_by_xpath('//*[@id=\"main\"]/div/div[2]/div[2]/div/div/div/div/div/ul/li['+str(i)+']/a')\n","    href = element.get_attribute('href')\n","    links.append(href)\n","    \n"," # iteramos sobre la lista de links para descargar los archivos con wget   \n","# for href in links:\n","#     wget.download(href)\n","#     time.sleep(3)"],"id":"chief-adobe","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"intellectual-proportion"},"source":["## Scraping con Selenium para obtener toda la serie historica de consumo de F&H en el MAPA 2013-2020"],"id":"intellectual-proportion"},{"cell_type":"markdown","metadata":{"id":"ranking-fifteen"},"source":["### Scraping Hortalizas MAPA"],"id":"ranking-fifteen"},{"cell_type":"code","metadata":{"id":"bacterial-aluminum"},"source":["# Preprocesado de etiquetas necesarias para sacar el consumo de hortalizas\n","# Entramos en la web a scrapear\n","browser.get('https://www.mapa.gob.es/app/consumo-en-hogares/resultado.asp?CCAA=1&AA=11&periodo=1&grupo=16')\n","\n","# se saca la etiqueta que contiene un menú desplegable con las diferentes comunidades autonomas\n","select = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/form/fieldset/p[1]/select\")          \n","options = select.find_elements_by_tag_name(\"option\") # se obtiene las diferentes opciones del menú desplegable\n","\n","optionsList = []\n","optionsPeriod = list(range(1,91,1))\n","\n","## se le asigna un valor a cada comunidad para luego poder iterar sobre estos y sacar de manera automatica\n","#los datos por comunidad\n","for option in options: \n","    optionsList.append(option.get_attribute(\"value\"))\n","    \n","optionsList.pop(0)\n","\n","lista_url= []\n","## se construye la url teniendo en cuenta el periodo (90 semanas a Scrapear) y la comunidad autonoma \n","#y se guardan los links definitivos en una lista\n","for optionValue in optionsList:\n","    url = 'https://www.mapa.gob.es/app/consumo-en-hogares/resultado.asp?CCAA='+str(optionValue)\n","    for optionPeriod in optionsPeriod:\n","        url2 = url+'&AA=11&periodo='+str(optionPeriod)+'&grupo=16'\n","        lista_url.append(url2)"],"id":"bacterial-aluminum","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"cellular-compilation"},"source":["#Este codigo se recorre los links generados, los abre con selenium, scrapea las etiquetas y la tabla de datos y los guarda en un csv\n","start_loop = time.time()\n","for url in lista_url:\n","    browser.get(url)\n","    time.sleep(3)\n","    for optionPeriod in optionsPeriod:\n","        time.sleep(2)\n","        try:\n","            fecha = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/div[10]/div/strong[2]\").text\n","            ccaa_scrap = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/div[8]/div/strong[2]\").text\n","            tables = pd.read_html(url)\n","            df =tables[1]\n","            new_header = df.iloc[0] \n","            df = df[1:] \n","            df.columns = new_header\n","            df['CCAA'] = ccaa_scrap\n","            df['Fecha'] = fecha\n","            df['Año'] = df['Fecha'].str[0:5]\n","            df['Mes'] = df['Fecha'].str[7:]\n","            df.drop(columns=['Fecha'],inplace=True)\n","            df.to_csv(''+ccaa_scrap+'_hortalizas_'+str(fecha)+'.csv', index= False, encoding= 'utf-8-sig')\n","            print(\"Se ha terminado de scrapear {} en el periodo {}\".format(ccaa_scrap,fecha))\n","            break\n","        except:\n","            print(\"¡No se puedo extraer la fecha!\")\n","            break\n","        \n","end_loop = time.time()\n","\n","print(\"¡Se ha Scrapeado todos los datos de consumo de hortalizas satisfactoriamente en {} horas!\".format(((end_loop - start_loop)/60)/60))"],"id":"cellular-compilation","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"musical-boutique"},"source":["### Scraping frutas frescas MAPA"],"id":"musical-boutique"},{"cell_type":"code","metadata":{"id":"freelance-adapter"},"source":["# Preprocesado de etiquetas necesarias para sacar el consumo de hortalizas\n","# Entramos en la web a scrapear\n","browser.get('https://www.mapa.gob.es/app/consumo-en-hogares/resultado.asp?CCAA=1&AA=11&periodo=1&grupo=17')\n","\n","# se saca la etiqueta que contiene un menú desplegable con las diferentes comunidades autonomas\n","select = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/form/fieldset/p[1]/select\")          \n","options = select.find_elements_by_tag_name(\"option\") # se obtiene las diferentes opciones del menú desplegable\n","\n","optionsList = []\n","optionsPeriod = list(range(1,91,1))\n","\n","## se le asigna un valor a cada comunidad para luego poder iterar sobre estos y sacar de manera automatica\n","#los datos por comunidad\n","for option in options: \n","    optionsList.append(option.get_attribute(\"value\"))\n","    \n","optionsList.pop(0)\n","\n","lista_url= []\n","## se construye la url teniendo en cuenta el periodo (90 semanas a Scrapear) y la comunidad autonoma \n","#y se guardan los links definitivos en una lista\n","for optionValue in optionsList:\n","    url = 'https://www.mapa.gob.es/app/consumo-en-hogares/resultado.asp?CCAA='+str(optionValue)\n","    for optionPeriod in optionsPeriod:\n","        url2 = url+'&AA=11&periodo='+str(optionPeriod)+'&grupo=17'\n","        lista_url.append(url2)"],"id":"freelance-adapter","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sublime-profession"},"source":["#Scraping frutas frescas\n","start_loop = time.time()\n","for url in lista_url:\n","    browser.get(url)\n","    time.sleep(4)\n","    for optionPeriod in optionsPeriod:\n","        time.sleep(3)\n","        try:\n","            fecha = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/div[10]/div/strong[2]\").text\n","            ccaa_scrap = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/div[8]/div/strong[2]\").text\n","            tables = pd.read_html(url)\n","            df =tables[1]\n","            new_header = df.iloc[0] \n","            df = df[1:] \n","            df.columns = new_header\n","            df['CCAA'] = ccaa_scrap\n","            df['Fecha'] = fecha\n","            df['Año'] = df['Fecha'].str[0:5]\n","            df['Mes'] = df['Fecha'].str[7:]\n","            df.drop(columns=['Fecha'],inplace=True)\n","            df.to_csv(''+ccaa_scrap+'_frutas_'+str(fecha)+'.csv', index= False, encoding= 'utf-8-sig')\n","            print(\"Se ha terminado de scrapear {} en el periodo {}\".format(ccaa_scrap,fecha))\n","            break\n","        except:\n","            print(\"¡No se puedo extraer el periodo! {}\".format(url))\n","            break\n","        \n","end_loop = time.time()\n","\n","print(\"¡Se ha Scrapeado todos los datos de consumo de frutas satisfactoriamente en {} horas!\".format(((end_loop - start_loop)/60)/60))\n","    "],"id":"sublime-profession","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"front-kenya"},"source":["## Scraping Hortalizas por Canal de Venta (Hipermercado, Internet, Supermercado)"],"id":"front-kenya"},{"cell_type":"code","metadata":{"id":"applied-playlist"},"source":["### Hipermercados, Supermercados y Internet\n","# Preprocesado de etiquetas necesarias para sacar las hortalizas\n","# Entramos en la web a scrapear\n","\n","browser.get('https://www.mapa.gob.es/app/consumo-en-hogares/resultadoes.asp?lugar=105&AA=11&periodo=1&grupo=16')\n","\n","optionsVenta = [105,104,123]\n","\n","optionsPeriod = list(range(1,91,1))\n","\n","## se le asigna un valor a cada comunidad para luego poder iterar sobre estos y sacar de manera automaticá \n","#los datos por comunidad\n","\n","lista_url= []\n","## se construye la url teniendo en cuenta el periodo (90 semanas a Scrapear) y la comunidad autonoma \n","#y se guardan los links definitivos en una lista\n","for optionVenta in optionsVenta:\n","    url = 'https://www.mapa.gob.es/app/consumo-en-hogares/resultadoes.asp?lugar='+str(optionVenta)\n","    for optionPeriod in optionsPeriod:\n","        url2 = url+'&AA=11&periodo='+str(optionPeriod)+'&grupo=16'\n","        lista_url.append(url2)"],"id":"applied-playlist","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"imperial-scholar"},"source":["start_loop = time.time()\n","counter = 0\n","#Nos reccoremos la lista de urls generadas\n","for url in lista_url:\n","    browser.get(url)\n","    fecha = browser.find_element_by_xpath('//*[@id=\"app_section\"]/div[10]/div/strong[2]').text\n","    canal_scrap = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/div[8]/div/strong[2]\").text\n","    if canal_scrap == 'SUPER/AUTOS/G.ALM.':\n","        canal_scrap = 'SUPERMERCADO'\n","    else:\n","        pass\n","    time.sleep(3)\n","    #Nos recorremos los periodos correspondientes a los 7 años para que vaya iterando correctamente\n","    for optionPeriod in optionsPeriod:\n","        try:\n","            time.sleep(2)\n","            # Se sacan las etiquetas de fecha y comunidad autonoma para luego ser agregadas al dataframe resultante\n","            tables = pd.read_html(url)\n","            # Limpiamos el header y usamos el primer row como header, ya que es el más descriptivo\n","            df =tables[1]\n","            new_header = df.iloc[0] \n","            df = df[1:] \n","            df.columns = new_header\n","            # se agregan al dataframe las etiquetas mencionadas\n","            df['Canal_Venta'] = canal_scrap\n","            df['Fecha'] = fecha\n","            df['Año'] = df['Fecha'].str[0:5]\n","            df['Mes'] = df['Fecha'].str[7:]\n","            df.drop(columns=['Fecha'],inplace=True)\n","            df.to_csv(''+canal_scrap+'_ventas_'+str(fecha)+'.csv', index= False, encoding= 'utf-8-sig')\n","            counter += 1\n","            print(\"Iteraccion: {} . Se ha terminado de scrapear {} en el periodo {}\".format(counter,canal_scrap,fecha))\n","            break\n","        except:\n","            print(\"¡No se puedo extraer el periodo! {}\".format(url))\n","            break\n","        \n","end_loop = time.time()\n","\n","print(\"¡Se ha Scrapeado todos los datos de consumo satisfactoriamente en {} minutos!\".format((end_loop - start_loop)/60/60))"],"id":"imperial-scholar","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"spectacular-canada"},"source":["## Scraping Frutas por Canal de Venta (Hipermercado, Internet, Supermercado)"],"id":"spectacular-canada"},{"cell_type":"code","metadata":{"id":"introductory-heath"},"source":["### Hipermercados, Supermercados y Internet\n","# Preprocesado de etiquetas necesarias para sacar las hortalizas\n","# Entramos en la web a scrapear\n","\n","browser.get('https://www.mapa.gob.es/app/consumo-en-hogares/resultadoes.asp?lugar=105&AA=11&periodo=1&grupo=17')\n","\n","optionsVenta = [105,104,123]\n","\n","optionsPeriod = list(range(1,91,1))\n","\n","## se le asigna un valor a cada comunidad para luego poder iterar sobre estos y sacar de manera automaticá \n","#los datos por comunidad\n","\n","lista_url= []\n","## se construye la url teniendo en cuenta el periodo (90 semanas a Scrapear) y la comunidad autonoma \n","#y se guardan los links definitivos en una lista\n","for optionVenta in optionsVenta:\n","    url = 'https://www.mapa.gob.es/app/consumo-en-hogares/resultadoes.asp?lugar='+str(optionVenta)\n","    for optionPeriod in optionsPeriod:\n","        url2 = url+'&AA=11&periodo='+str(optionPeriod)+'&grupo=17'\n","        lista_url.append(url2)"],"id":"introductory-heath","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"streaming-electronics"},"source":["start_loop = time.time()\n","counter = 0\n","#Nos reccoremos la lista de urls generadas\n","for url in lista_url:\n","    browser.get(url)\n","    fecha = browser.find_element_by_xpath('//*[@id=\"app_section\"]/div[10]/div/strong[2]').text\n","    canal_scrap = browser.find_element_by_xpath(\"/html/body/div[2]/div/div/div[2]/div/div[2]/div/div[8]/div/strong[2]\").text\n","    if canal_scrap == 'SUPER/AUTOS/G.ALM.':\n","        canal_scrap = 'SUPERMERCADO'\n","    else:\n","        pass\n","    time.sleep(3)\n","    #Nos recorremos los periodos correspondientes a los 7 años para que vaya iterando correctamente\n","    for optionPeriod in optionsPeriod:\n","        try:\n","            time.sleep(2)\n","            # Se sacan las etiquetas de fecha y comunidad autonoma para luego ser agregadas al dataframe resultante\n","            tables = pd.read_html(url)\n","            # Limpiamos el header y usamos el primer row como header, ya que es el más descriptivo\n","            df =tables[1]\n","            new_header = df.iloc[0] \n","            df = df[1:] \n","            df.columns = new_header\n","            # se agregan al dataframe las etiquetas mencionadas\n","            df['Canal_Venta'] = canal_scrap\n","            df['Fecha'] = fecha\n","            df['Año'] = df['Fecha'].str[0:5]\n","            df['Mes'] = df['Fecha'].str[7:]\n","            df.drop(columns=['Fecha'],inplace=True)\n","            df.to_csv(''+canal_scrap+'_ventas_'+str(fecha)+'.csv', index= False, encoding= 'utf-8-sig')\n","            counter += 1\n","            print(\"Iteraccion: {} . Se ha terminado de scrapear {} en el periodo {}\".format(counter,canal_scrap,fecha))\n","            break\n","        except:\n","            print(\"¡No se puedo extraer la fecha! {}\".format(url))\n","            break\n","        \n","end_loop = time.time()\n","\n","print(\"¡Se ha Scrapeado todos los datos de consumo satisfactoriamente en {} minutos!\".format((end_loop - start_loop)/60/60))"],"id":"streaming-electronics","execution_count":null,"outputs":[]}]}